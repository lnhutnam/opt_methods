#!/usr/bin/env julia
# Comprehensive example showcasing all converted loss functions

using LinearAlgebra, Random, Statistics
include("../src/OptMethods.jl")
using .OptMethods

# Set random seed for reproducibility
Random.seed!(42)

println("Loss Functions Comprehensive Comparison")
println("=" ^ 60)

# Problem dimensions
n_samples, n_features = 100, 10

# Generate synthetic data
println("Generating synthetic data...")
println("  Samples: $n_samples, Features: $n_features")

# Design matrix
A = randn(n_samples, n_features)
A = A ./ norm.(eachrow(A))  # Normalize rows

# True parameter
x_true = randn(n_features)

# Generate targets
y_linear = A * x_true + 0.1 * randn(n_samples)  # Linear regression targets
y_binary = Float64.(A * x_true + 0.1 * randn(n_samples) .> 0)  # Binary classification

println("  Linear targets range: [$(round(minimum(y_linear), digits=2)), $(round(maximum(y_linear), digits=2))]")
println("  Binary targets: $(sum(y_binary)) positive, $(sum(1 .- y_binary)) negative")

# Starting point
x0 = randn(n_features)

println("\n" * "=" ^ 60)
println("LOSS FUNCTION COMPARISON")
println("=" ^ 60)

# Test each loss function
loss_functions = []

# 1. Linear Regression
println("\n1. LINEAR REGRESSION")
println("-" ^ 30)
try
    loss_lr = LinearRegression(A, y_linear; l2=0.01)

    # Compute various metrics
    val_0 = value(loss_lr, x0)
    grad_0 = gradient(loss_lr, x0)
    hess_0 = hessian(loss_lr, x0)
    smooth = smoothness(loss_lr)

    push!(loss_functions, ("Linear Regression", loss_lr, val_0, norm(grad_0), smooth))

    println("  âœ“ Function value at x0: $(round(val_0, digits=6))")
    println("  âœ“ Gradient norm at x0: $(round(norm(grad_0), digits=6))")
    println("  âœ“ Hessian condition number: $(round(cond(hess_0), digits=2))")
    println("  âœ“ Smoothness constant: $(round(smooth, digits=6))")

    # Test stochastic gradient
    batch_size = 10
    stoch_grad = stochastic_gradient(loss_lr, x0; batch_size=batch_size)
    println("  âœ“ Stochastic gradient norm (batch=$batch_size): $(round(norm(stoch_grad), digits=6))")

catch e
    println("  âœ— Error: $e")
end

# 2. Logistic Regression
println("\n2. LOGISTIC REGRESSION")
println("-" ^ 30)
try
    loss_logistic = LogisticRegression(A, y_binary; l2=0.01)

    # Compute various metrics
    val_0 = value(loss_logistic, x0)
    grad_0 = gradient(loss_logistic, x0)
    hess_0 = hessian(loss_logistic, x0)
    smooth = smoothness(loss_logistic)

    push!(loss_functions, ("Logistic Regression", loss_logistic, val_0, norm(grad_0), smooth))

    println("  âœ“ Function value at x0: $(round(val_0, digits=6))")
    println("  âœ“ Gradient norm at x0: $(round(norm(grad_0), digits=6))")
    println("  âœ“ Hessian condition number: $(round(cond(Matrix(hess_0)), digits=2))")
    println("  âœ“ Smoothness constant: $(round(smooth, digits=6))")

    # Test additional properties
    max_smooth = max_smoothness(loss_logistic)
    avg_smooth = average_smoothness(loss_logistic)
    println("  âœ“ Maximum smoothness: $(round(max_smooth, digits=6))")
    println("  âœ“ Average smoothness: $(round(avg_smooth, digits=6))")

    # Test stochastic gradient
    stoch_grad = stochastic_gradient(loss_logistic, x0; batch_size=10)
    println("  âœ“ Stochastic gradient norm: $(round(norm(stoch_grad), digits=6))")

catch e
    println("  âœ— Error: $e")
end

# 3. Log-Sum-Exp
println("\n3. LOG-SUM-EXP")
println("-" ^ 30)
try
    # Create log-sum-exp with random data
    A_lse = 0.1 * randn(n_samples, n_features)  # Smaller scale for stability
    b_lse = randn(n_samples)

    loss_lse = LogSumExp(; max_smoothing=1.0, least_squares_term=false,
                        A=A_lse, b=b_lse, l2=0.01)

    # Compute various metrics
    val_0 = value(loss_lse, x0)
    grad_0 = gradient(loss_lse, x0)
    hess_0 = hessian(loss_lse, x0)
    smooth = smoothness(loss_lse)

    push!(loss_functions, ("Log-Sum-Exp", loss_lse, val_0, norm(grad_0), smooth))

    println("  âœ“ Function value at x0: $(round(val_0, digits=6))")
    println("  âœ“ Gradient norm at x0: $(round(norm(grad_0), digits=6))")
    println("  âœ“ Hessian condition number: $(round(cond(hess_0), digits=2))")
    println("  âœ“ Smoothness constant: $(round(smooth, digits=6))")

    # Test with least squares term
    loss_lse_ls = LogSumExp(; max_smoothing=1.0, least_squares_term=true,
                           A=A_lse, b=b_lse, l2=0.01)
    val_ls = value(loss_lse_ls, x0)
    println("  âœ“ With least squares term: $(round(val_ls, digits=6))")

catch e
    println("  âœ— Error: $e")
end

# 4. Regularizers
println("\n4. REGULARIZERS")
println("-" ^ 30)

# Standard L1/L2 regularizer
try
    reg_standard = Regularizer(l1=0.1, l2=0.05)
    reg_val = value(reg_standard, x0)
    reg_prox = prox(reg_standard, x0, 0.1)

    println("  âœ“ L1/L2 Regularizer value: $(round(reg_val, digits=6))")
    println("  âœ“ Proximal operator norm: $(round(norm(reg_prox), digits=6))")

catch e
    println("  âœ— L1/L2 Regularizer error: $e")
end

# Bounded L2 regularizer
try
    reg_bounded = BoundedL2Regularizer(1.0)
    bounded_val = value(reg_bounded, x0)
    bounded_grad = grad(reg_bounded, x0)
    bounded_smooth = smoothness(reg_bounded)

    println("  âœ“ Bounded L2 value: $(round(bounded_val, digits=6))")
    println("  âœ“ Bounded L2 gradient norm: $(round(norm(bounded_grad), digits=6))")
    println("  âœ“ Bounded L2 smoothness: $(round(bounded_smooth, digits=6))")

catch e
    println("  âœ— Bounded L2 Regularizer error: $e")
end

# OPTIMIZATION COMPARISON
println("\n" * "=" ^ 60)
println("OPTIMIZATION COMPARISON")
println("=" ^ 60)

for (name, loss, val_0, grad_norm_0, smooth) in loss_functions
    println("\nOptimizing $name...")

    try
        # Use gradient descent with adaptive step size
        lr = 1.0 / smooth  # Inverse smoothness constant
        optimizer = GradientDescent(loss; lr=lr, trace_len=50)

        # Run optimization
        trace = run!(optimizer, copy(x0); it_max=100, tqdm_iterations=false)

        # Get final results
        final_val = best_loss_value(trace)
        iterations = length(trace.its_all[first(keys(trace.its_all))])

        improvement = val_0 - final_val

        println("  âœ“ Initial value: $(round(val_0, digits=6))")
        println("  âœ“ Final value: $(round(final_val, digits=6))")
        println("  âœ“ Improvement: $(round(improvement, digits=6))")
        println("  âœ“ Iterations: $iterations")
        println("  âœ“ Step size used: $(round(lr, digits=6))")

    catch e
        println("  âœ— Optimization error: $e")
    end
end

println("\n" * "=" ^ 60)
println("SUMMARY")
println("=" ^ 60)

println("\nâœ… Successfully converted and tested all loss functions:")
println("   â€¢ Linear Regression - Least squares with L2 regularization")
println("   â€¢ Logistic Regression - Binary classification with sigmoid")
println("   â€¢ Log-Sum-Exp - Smooth maximum function with optional least squares")
println("   â€¢ L1/L2 Regularizer - Standard sparsity and ridge penalties")
println("   â€¢ Bounded L2 Regularizer - Nonconvex smooth penalty")

println("\nâœ… All loss functions provide:")
println("   â€¢ Function evaluation")
println("   â€¢ Gradient computation")
println("   â€¢ Hessian computation (where applicable)")
println("   â€¢ Smoothness constants")
println("   â€¢ Stochastic gradient computation")
println("   â€¢ Integration with optimization algorithms")

println("\nðŸŽ¯ All Python loss functions successfully converted to Julia!")
println("   Ready for high-performance optimization workflows.")

println("\n" * "=" ^ 60)